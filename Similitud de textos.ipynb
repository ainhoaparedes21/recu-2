{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/dreguera/.local/lib/python3.8/site-packages (3.5)\r\n",
      "Requirement already satisfied: regex in /home/dreguera/.local/lib/python3.8/site-packages (from nltk) (2020.11.13)\r\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\r\n",
      "Requirement already satisfied: tqdm in /home/dreguera/.local/lib/python3.8/site-packages (from nltk) (4.54.1)\r\n",
      "Requirement already satisfied: joblib in /home/dreguera/.local/lib/python3.8/site-packages (from nltk) (0.17.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language toolkit (NLTK) es la biblioteca más popular para el procesamiento de lenguaje natural (NLP) en Python. Contiene librerías de procesamiento de texto para tokenización, análisis sintáctico, clasificación, derivación, etiquetado y razonamiento semántico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/dreguera/.local/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/dreguera/.local/lib/python3.8/site-packages (from gensim) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/dreguera/.local/lib/python3.8/site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/dreguera/.local/lib/python3.8/site-packages (from gensim) (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim es una librería Procesamiento de Lenguaje Natural (NLP) para procesar textos, trabajando con modelos de vectores de palabras (como Word2Vec, FastText, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dreguera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dreguera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"La Inteligencia Aritifical son sistemas que permiten dotar a las máquinas de inteligencia para imitar destrezas humanas.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el método word_tokenize() para dividir una frase en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La Inteligencia Aritifical son sistemas que permiten dotar a las máquinas de inteligencia para imitar destrezas humanas.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'Inteligencia', 'Aritifical', 'son', 'sistemas', 'que', 'permiten', 'dotar', 'a', 'las', 'máquinas', 'de', 'inteligencia', 'para', 'imitar', 'destrezas', 'humanas', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "with open ('demo.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el archivo demo.txt y agregamos frases con token en la matriz para la conversión de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['con',\n",
       "  'las',\n",
       "  'redes',\n",
       "  'neuronales',\n",
       "  'artificiales',\n",
       "  'obtenemos',\n",
       "  'conocimiento',\n",
       "  'de',\n",
       "  'grandes',\n",
       "  'volúmenes',\n",
       "  'de',\n",
       "  'datos',\n",
       "  '.'],\n",
       " ['la',\n",
       "  'sistemas',\n",
       "  'expertos',\n",
       "  'nos',\n",
       "  'permite',\n",
       "  'simular',\n",
       "  'destrezas',\n",
       "  'humanas',\n",
       "  'mediante',\n",
       "  'algormitmos',\n",
       "  'inteligentes',\n",
       "  '.'],\n",
       " ['la',\n",
       "  'inteligencia',\n",
       "  'artificial',\n",
       "  'va',\n",
       "  'desde',\n",
       "  'sistemas',\n",
       "  'expertos',\n",
       "  'hasta',\n",
       "  'algoritmos',\n",
       "  'que',\n",
       "  'aprenden',\n",
       "  'de',\n",
       "  'los',\n",
       "  'datos',\n",
       "  '.']]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un diccionario mapea a cada palabra con un número. Gensim te permite leer el texto y actualizar el diccionario, una línea a la vez, sin cargar todo el archivo de texto en la memoria del sistema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['con',\n",
       "  'las',\n",
       "  'redes',\n",
       "  'neuronales',\n",
       "  'artificiales',\n",
       "  'obtenemos',\n",
       "  'conocimiento',\n",
       "  'de',\n",
       "  'grandes',\n",
       "  'volúmenes',\n",
       "  'de',\n",
       "  'datos',\n",
       "  '.'],\n",
       " ['la',\n",
       "  'sistemas',\n",
       "  'expertos',\n",
       "  'nos',\n",
       "  'permite',\n",
       "  'simular',\n",
       "  'destrezas',\n",
       "  'humanas',\n",
       "  'mediante',\n",
       "  'algormitmos',\n",
       "  'inteligentes',\n",
       "  '.'],\n",
       " ['la',\n",
       "  'inteligencia',\n",
       "  'artificial',\n",
       "  'va',\n",
       "  'desde',\n",
       "  'sistemas',\n",
       "  'expertos',\n",
       "  'hasta',\n",
       "  'algoritmos',\n",
       "  'que',\n",
       "  'aprenden',\n",
       "  'de',\n",
       "  'los',\n",
       "  'datos',\n",
       "  '.']]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'artificiales': 1, 'con': 2, 'conocimiento': 3, 'datos': 4, 'de': 5, 'grandes': 6, 'las': 7, 'neuronales': 8, 'obtenemos': 9, 'redes': 10, 'volúmenes': 11, 'algormitmos': 12, 'destrezas': 13, 'expertos': 14, 'humanas': 15, 'inteligentes': 16, 'la': 17, 'mediante': 18, 'nos': 19, 'permite': 20, 'simular': 21, 'sistemas': 22, 'algoritmos': 23, 'aprenden': 24, 'artificial': 25, 'desde': 26, 'hasta': 27, 'inteligencia': 28, 'los': 29, 'que': 30, 'va': 31}\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 2),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1)],\n",
       " [(0, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1)],\n",
       " [(0, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (14, 1),\n",
       "  (17, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1)]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['artificiales', 0.32], ['con', 0.32], ['conocimiento', 0.32], ['datos', 0.12], ['de', 0.24], ['grandes', 0.32], ['las', 0.32], ['neuronales', 0.32], ['obtenemos', 0.32], ['redes', 0.32], ['volúmenes', 0.32]]\n",
      "[['algormitmos', 0.34], ['destrezas', 0.34], ['expertos', 0.13], ['humanas', 0.34], ['inteligentes', 0.34], ['la', 0.13], ['mediante', 0.34], ['nos', 0.34], ['permite', 0.34], ['simular', 0.34], ['sistemas', 0.13]]\n",
      "[['datos', 0.12], ['de', 0.12], ['expertos', 0.12], ['la', 0.12], ['sistemas', 0.12], ['algoritmos', 0.32], ['aprenden', 0.32], ['artificial', 0.32], ['desde', 0.32], ['hasta', 0.32], ['inteligencia', 0.32], ['los', 0.32], ['que', 0.32], ['va', 0.32]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('a/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n"
     ]
    }
   ],
   "source": [
    "file2_docs = []\n",
    "\n",
    "with open ('demo2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.04221008 0.04529131 0.99999994]\n"
     ]
    }
   ],
   "source": [
    "print('Comparing Result:', sims[query_doc_tf_idf]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0875013\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity float: 0.3625004291534424\n",
      "Average similarity percentage: 36.25004291534424\n",
      "Average similarity rounded percentage: 36\n"
     ]
    }
   ],
   "source": [
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
